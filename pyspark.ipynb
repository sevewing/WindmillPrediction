{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37264bit3a462acd451f4b989328b3d637ce9031",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, udf\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import schemas\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Methods\n",
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    return sqlContext.read.schema(schema).parquet(path) if schema is not None else sqlContext.read.parquet(path)\n",
    "\n",
    "def load_parquet_from_weather_toPandas(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    weather = load_parquet(path) \\\n",
    "                    .withColumnRenamed(\"__index_level_0__\", \"TIME\") \\\n",
    "                    .dropna() \\\n",
    "                    .withColumn(\"id\", monotonically_increasing_id())\n",
    "    weather.createOrReplaceTempView(\"weather_temp\")\n",
    "    weather_dic = spark.sql(\"select * from weather_temp where id in (select max(id) as id from weather_temp group by TIME)\").toPandas()\n",
    "    return weather_dic\n",
    "\n",
    "def load_csv(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load csv file\n",
    "    \"\"\"\n",
    "    # return sqlContext.read.schema(schema).csv(path, sep=\";\", header=True, schema=schema) if schema is not None else sqlContext.read.schema(schema).csv(path, sep=\";\", header=True)\n",
    "    return sqlContext.read.csv(path, sep=\";\", header=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_by_grid(df:pd.DataFrame, type = FloatType()):\n",
    "    return udf(lambda g, t: list(df[df['TIME'] == t[:14]+'00:00'][g])[0], type)\n",
    "\n",
    "def udf_by_ws():\n",
    "    schema = StructType([\n",
    "        StructField(\"u_interp\", FloatType(), True),\n",
    "        StructField(\"v_interp\", FloatType(), True)\n",
    "    ])\n",
    "    return udf(lambda s1, d1, s2, d2, z: tools.wind_interp(s1, d1, s2, d2, z), schema)\n",
    "\n",
    "def udf_by_tmp():\n",
    "    schema = StructType([\n",
    "        StructField(\"tmp_interp\", FloatType(), True),\n",
    "    ])\n",
    "    return udf(lambda t1, t2, z: tools.tmp_interp(t1, t2, z), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_regist():\n",
    "    udf_type = udf(lambda x: {\"H\": 1.0, \"W\": 2.0, \"P\": 3.0, \"M\": 4.0}.get(x, 0.0), FloatType())\n",
    "    udf_placement = udf(lambda x: {\"LAND\": 1.0, \"HAV\": 2.0}.get(x, 0.0), FloatType())\n",
    "\n",
    "    udf_hour = udf(lambda x: int(x[11:13]), IntegerType())\n",
    "    udf_ws10  = udf_by_grid(ws10_dic, FloatType())\n",
    "    udf_ws100  = udf_by_grid(ws100_dic, FloatType())\n",
    "    udf_wd10  = udf_by_grid(wd10_dic, IntegerType())\n",
    "    udf_wd100  = udf_by_grid(wd100_dic, IntegerType())\n",
    "    # udf_tmp2  = udf_by_grid(tmp2_dic, IntegerType())\n",
    "    # udf_tmp100  = udf_by_grid(tmp100_dic, IntegerType())\n",
    "    udf_ws_interp  = udf_by_ws()\n",
    "    # udf_tmp_interp  = udf_by_tmp()\n",
    "    return udf_type, udf_placement,udf_hour, udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, join_df):\n",
    "    df = df.join(join_df, on=\"GSRN\") \\\n",
    "                    .select(df.GSRN, df.TIME_CET, join_df.cluster, df.VAERDI, join_df.Navhub_height, join_df.grid)\n",
    "\n",
    "    df = df.withColumn(\"hour\", udf_hour(df.TIME_CET)) \\\n",
    "                .withColumn(\"ws10\", udf_ws10(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"ws100\", udf_ws100(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"wd10\", udf_wd10(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"wd100\", udf_wd100(df.grid, df.TIME_CET))\n",
    "\n",
    "                # .withColumn(\"tmp2\", udf_tmp2(df.grid, df.TIME_CET)) \\\n",
    "                # .withColumn(\"tmp100\", udf_tmp100(df.grid, df.TIME_CET))\n",
    "    return df\n",
    "\n",
    "# def aggregate_with_interp(df, join_df):\n",
    "#     df = aggregate(df, join_df)\n",
    "#     df = df.withColumn(\"wsCol\", \\\n",
    "#                 udf_ws_interp(df.ws10, df.wd10, df.ws100, df.wd100, df.Navhub_height)) \\\n",
    "#         .withColumn(\"tmpCol\", \\\n",
    "#         udf_tmp_interp(df.tmp2, df.tmp100, df.Navhub_height)) \\\n",
    "#                 .select(\"GSRN\", \"TIME_CET\", \"Placement\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\", \"VAERDI\", \"wsCol.u_interp\", \"wsCol.v_interp\", \"tmpCol.tmp_interp\")\n",
    "#     return df\n",
    "\n",
    "def aggregate_with_interp(df, join_df):\n",
    "    df = aggregate(df, join_df)\n",
    "    df = df.withColumn(\"wsCol\", \\\n",
    "                udf_ws_interp(df.ws10, df.wd10, df.ws100, df.wd100, df.Navhub_height)) \\\n",
    "                .select(\"GSRN\", \"TIME_CET\", \"hour\", \"cluster\", \"VAERDI\", \"wsCol.u_interp\", \"wsCol.v_interp\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sparkContext\\\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"WindTurbine\") \\\n",
    "    .config(\"spark.executor.memory\", \"8gb\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settlement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[GSRN: string, VAERDI: float, TIME_CET: string]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # to read parquet file\n",
    "settlement = load_parquet(\"data/ITU_DATA/settlement/2019.parquet\", schemas.settlement_schema)\n",
    "settlement = settlement.dropna(subset =[\"VAERDI\"]) \\\n",
    "            .withColumn(\"VAERDI\", settlement[\"VAERDI\"].cast(\"float\"))\n",
    "            # .where(\"TIME_CET like '%:00:%'\")\n",
    "settlement.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- GSRN: string (nullable = true)\n |-- VAERDI: float (nullable = true)\n |-- TIME_CET: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "settlement.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weather\n",
    "from ENetNEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_10m.parquet\")\n",
    "ws100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_100m.parquet\")\n",
    "wd10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_10m.parquet\")\n",
    "wd100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_100m.parquet\")\n",
    "# tmp2_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/temperatur_2m.parquet\")\n",
    "# tmp100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/temperatur_100m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_type, udf_placement, udf_hour, udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp = udf_regist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windmills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[GSRN: string, Turbine_type: float, Parent_GSRN: string, BBR_municipal: string, Placement: float, UTM_x: string, UTM_y: string, Capacity_kw: float, Rotor_diameter: float, Navhub_height: float, grid: string, grid_in_range: string]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windmill = load_csv(\"data/windmill_cleaned.csv\", schemas.windmills_schema)\n",
    "windmill = windmill.where(\"grid != 0\") \\\n",
    "            .fillna(0.1) \\\n",
    "            .withColumn(\"Turbine_type\", udf_type(windmill.Turbine_type)) \\\n",
    "            .withColumn(\"Placement\", udf_placement(windmill.Placement))\n",
    "windmill.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- GSRN: string (nullable = true)\n |-- Turbine_type: float (nullable = true)\n |-- Parent_GSRN: string (nullable = true)\n |-- BBR_municipal: string (nullable = true)\n |-- Placement: float (nullable = true)\n |-- UTM_x: string (nullable = true)\n |-- UTM_y: string (nullable = true)\n |-- Capacity_kw: float (nullable = false)\n |-- Rotor_diameter: float (nullable = false)\n |-- Navhub_height: float (nullable = false)\n |-- grid: string (nullable = true)\n |-- grid_in_range: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "windmill.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.ML Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windmill self clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"Turbine_type\", \"Placement\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "windmill = vec_assembler.transform(windmill).select(\"GSRN\", \"features\", \"Navhub_height\",\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(windmill)\n",
    "windmill = scalerModel.transform(windmill)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal k = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "clusters = []\n",
    "\n",
    "for k in range(30, 50):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k, seed=1)\n",
    "    model = kmeans.fit(windmill)\n",
    "    cost.append(model.computeCost(windmill))\n",
    "    clusters.append(k)\n",
    "\n",
    "# Plot the cost\n",
    "df_cost = pd.DataFrame(cost)\n",
    "df_cost.columns = [\"cost\"]\n",
    "df_cost.insert(0, 'cluster', clusters)\n",
    "\n",
    "import pylab as pl\n",
    "pl.plot(df_cost.cluster, df_cost.cost)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[GSRN: string, cluster: int, Navhub_height: float, grid: string]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmean = KMeans(featuresCol='scaledFeatures', k=35, seed=1)\n",
    "model = kmean.fit(windmill)\n",
    "windmill = model.transform(windmill).withColumnRenamed(\"prediction\", \"cluster\").select(\"GSRN\", \"cluster\",\"Navhub_height\", \"grid\")\n",
    "windmill.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windmill.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicte with Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = settlement.where(\"TIME_CET != '2019-03-31 02:00:00'\").where(\"TIME_CET not like '2019-12-30 23%'\").sample(fraction=0.0003, seed=5)\n",
    "test = settlement.where(\"TIME_CET != '2019-03-31 02:00:00'\").where(\"TIME_CET like '2019-12-30 23%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = aggregate_with_interp(train, windmill)\n",
    "test = aggregate_with_interp(test, windmill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "46296"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"hour\", \"cluster\", \"u_interp\", \"v_interp\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "train = vec_assembler.transform(train).select(\"GSRN\",\"TIME_CET\", \"features\", \"VAERDI\")\n",
    "test = vec_assembler.transform(test).select(\"GSRN\",\"TIME_CET\", \"features\", \"VAERDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = train.select(\"grid\", \"TIME_CET\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2019-03-31 02:00:00 1026\n2019-03-31 02:00:00 943\n"
    }
   ],
   "source": [
    "for i in range(48060):\n",
    "    try:\n",
    "        list(ws10_dic[ws10_dic['TIME'] == ttt.loc[i]['TIME_CET']][ttt.loc[i]['grid']])[0]\n",
    "    except:\n",
    "        print(ttt.loc[i]['TIME_CET'],ttt.loc[i]['grid'])\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>grid</th>\n      <th>TIME_CET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1190</td>\n      <td>2019-01-01 03:00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1106</td>\n      <td>2019-01-01 21:00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1151</td>\n      <td>2019-01-01 03:00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>949</td>\n      <td>2019-01-01 05:00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1107</td>\n      <td>2019-01-01 06:00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48056</th>\n      <td>1077</td>\n      <td>2019-12-30 17:00:00</td>\n    </tr>\n    <tr>\n      <th>48057</th>\n      <td>898</td>\n      <td>2019-12-30 16:00:00</td>\n    </tr>\n    <tr>\n      <th>48058</th>\n      <td>943</td>\n      <td>2019-12-29 17:00:00</td>\n    </tr>\n    <tr>\n      <th>48059</th>\n      <td>1157</td>\n      <td>2019-12-25 18:00:00</td>\n    </tr>\n    <tr>\n      <th>48060</th>\n      <td>1157</td>\n      <td>2019-12-28 17:00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>48061 rows × 2 columns</p>\n</div>",
      "text/plain": "       grid             TIME_CET\n0      1190  2019-01-01 03:00:00\n1      1106  2019-01-01 21:00:00\n2      1151  2019-01-01 03:00:00\n3       949  2019-01-01 05:00:00\n4      1107  2019-01-01 06:00:00\n...     ...                  ...\n48056  1077  2019-12-30 17:00:00\n48057   898  2019-12-30 16:00:00\n48058   943  2019-12-29 17:00:00\n48059  1157  2019-12-25 18:00:00\n48060  1157  2019-12-28 17:00:00\n\n[48061 rows x 2 columns]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(train)\n",
    "train = scalerModel.transform(train)\n",
    "test = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1773.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 355.0 failed 1 times, most recent failure: Lost task 5.0 in stage 355.0 (TID 4687, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 899355 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:594)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:583)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:386)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:282)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:251)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$fit$1.apply(KMeans.scala:362)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$fit$1.apply(KMeans.scala:340)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-65c9c15ef8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1773.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 355.0 failed 1 times, most recent failure: Lost task 5.0 in stage 355.0 (TID 4687, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 899355 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:594)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.takeSample(RDD.scala:583)\n\tat org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:386)\n\tat org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:282)\n\tat org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:251)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$fit$1.apply(KMeans.scala:362)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$fit$1.apply(KMeans.scala:340)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "cost = []\n",
    "clusters = []\n",
    "\n",
    "for k in range(55, 70):\n",
    "    kmeans = KMeans(featuresCol='features',k=k, seed=1)\n",
    "    model = kmeans.fit(train)\n",
    "    cost.append(model.computeCost(train))\n",
    "    clusters.append(k)\n",
    "\n",
    "# Plot the cost\n",
    "df_cost = pd.DataFrame(cost)\n",
    "df_cost.columns = [\"cost\"]\n",
    "df_cost.insert(0, 'cluster', clusters)\n",
    "\n",
    "import pylab as pl\n",
    "pl.plot(df_cost.cluster, df_cost.cost)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal k = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(featuresCol='scaledFeatures', k=68, seed=1)\n",
    "model = kmean.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_avg = model.transform(train).where(\"VAERDI != 0.0\").groupBy(\"prediction\").avg(\"VAERDI\")\n",
    "pred_avg = pred_avg.withColumn(\"avg(VAERDI)\", pred_avg[\"avg(VAERDI)\"].cast(\"float\")).select(\"prediction\", \"avg(VAERDI)\")\n",
    "pred_test = model.transform(test).select(\"GSRN\",\"TIME_CET\",\"prediction\", \"VAERDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "result = pred_test.join(pred_avg, on=\"prediction\") \\\n",
    "        .groupby(\"TIME_CET\") \\\n",
    "        .agg(sf.sum(\"avg(VAERDI)\").alias(\"predicted\"), sf.sum(\"VAERDI\").alias(\"truth\")) \\\n",
    "        .orderBy(\"TIME_CET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = udf(lambda x,y : x-y, FloatType())\n",
    "result.withColumn(\"error\", error(result.truth, result.predicted)) \\\n",
    "        .select(\"TIME_CET\",\"predicted\",\"truth\",\"error\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=\"TIME_CET\", y=\"value\", data=pd.melt(data, ['TIME_CET']), hue='variable',linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}