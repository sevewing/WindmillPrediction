{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37264bit3a462acd451f4b989328b3d637ce9031",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, udf\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import schemas\n",
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    return sqlContext.read.schema(schema).parquet(path) if schema is not None else sqlContext.read.parquet(path)\n",
    "\n",
    "def load_parquet_from_weather_toPandas(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    weather = load_parquet(path) \\\n",
    "                    .withColumnRenamed(\"__index_level_0__\", \"TIME\") \\\n",
    "                    .dropna() \\\n",
    "                    .withColumn(\"id\", monotonically_increasing_id())\n",
    "    weather.createOrReplaceTempView(\"weather_temp\")\n",
    "    weather_dic = spark.sql(\"select * from weather_temp where id in (select max(id) as id from weather_temp group by TIME)\").toPandas()\n",
    "    return weather_dic\n",
    "\n",
    "def load_csv(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load csv file\n",
    "    \"\"\"\n",
    "    # return sqlContext.read.schema(schema).csv(path, sep=\";\", header=True, schema=schema) if schema is not None else sqlContext.read.schema(schema).csv(path, sep=\";\", header=True)\n",
    "    return sqlContext.read.csv(path, sep=\";\", header=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_by_grid(df:pd.DataFrame, type = DoubleType()):\n",
    "    return udf(lambda g, t: list(df[df['TIME'] == t][g])[0], type)\n",
    "def udf_by_ws():\n",
    "    schema = StructType([\n",
    "        StructField(\"u_interp\", DoubleType(), True),\n",
    "        StructField(\"v_interp\", DoubleType(), True)\n",
    "    ])\n",
    "    return udf(lambda s1, d1, s2, d2, z: tools.wind_interp(s1, d1, s2, d2, z), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_regist():\n",
    "    udf_ws10  = udf_by_grid(ws10_dic, DoubleType())\n",
    "    udf_ws100  = udf_by_grid(ws100_dic, DoubleType())\n",
    "    udf_wd10  = udf_by_grid(wd10_dic, IntegerType())\n",
    "    udf_wd100  = udf_by_grid(wd100_dic, IntegerType())\n",
    "    udf_ws_interp  = udf_by_ws()\n",
    "    return udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sparkContext\\\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"WindTurbine\") \\\n",
    "    .config(\"spark.executor.memory\", \"8gb\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # to read parquet file\n",
    "settlement = load_parquet(\"data/ITU_DATA/settlement/2019.parquet\", schemas.settlement_schema)\n",
    "settlement = settlement.dropna(subset =[\"VAERDI\"]) \\\n",
    "            .withColumn(\"VAERDI\", settlement[\"VAERDI\"].cast(\"double\")) \\\n",
    "            .where(\"TIME_CET like '%:00:%'\") \\\n",
    "\n",
    "settlement_last = settlement.where(\"TIME_CET like '2019-12-30%'\") \\\n",
    "\n",
    "# settlement = settlement.where(\"TIME_CET not like '2019-12-30%'\").sample(fraction=0.00001, seed=5)\n",
    "settlement = settlement.sample(fraction=0.00001, seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settlement.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "windmills = load_csv(\"data/windmill_cleaned.csv\", schemas.windmills_schema) \\\n",
    "            .dropna(subset =[\"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\"]) \\\n",
    "            .where(\"grid != 0\") \\\n",
    "            .where(\"Navhub_height != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windmills.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_10m.parquet\")\n",
    "ws100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_100m.parquet\")\n",
    "wd10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_10m.parquet\")\n",
    "wd100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_100m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp = udf_regist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df):\n",
    "    df = df.join(windmills, on=\"GSRN\") \\\n",
    "                    .select(\"GSRN\", \"TIME_CET\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\", \"VAERDI\", \"grid\")\n",
    "\n",
    "    df = df.withColumn(\"ws10\", udf_ws10(df.grid, df.TIME_CET)) \\\n",
    "            .withColumn(\"ws100\", udf_ws100(df.grid, df.TIME_CET)) \\\n",
    "            .withColumn(\"wd10\", udf_wd10(df.grid, df.TIME_CET)) \\\n",
    "            .withColumn(\"wd100\", udf_wd100(df.grid, df.TIME_CET))\n",
    "\n",
    "    df = df.withColumn(\"wsCol\", \\\n",
    "                udf_ws_interp(df.ws10, df.wd10, df.ws100, df.wd100, df.Navhub_height)) \\\n",
    "                .select(\"GSRN\", \"TIME_CET\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\", \"VAERDI\", \"wsCol.u_interp\", \"wsCol.v_interp\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = aggregate(settlement)\n",
    "test = aggregate(settlement_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\", \"u_interp\", \"v_interp\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "train_feat = vec_assembler.transform(train).select(\"TIME_CET\", \"features\", \"VAERDI\")\n",
    "\n",
    "test_feat = vec_assembler.transform(test).select(\"TIME_CET\", \"features\", \"VAERDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(train_feat)\n",
    "train_scaled = scalerModel.transform(train_feat)\n",
    "\n",
    "scalerModel = scaler.fit(test_feat)\n",
    "test_scaled = scalerModel.transform(test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(featuresCol='features', k=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmean.fit(train_feat)\n",
    "pred_agged = model.transform(train_feat).groupBy(\"prediction\").avg(\"VAERDI\").select(\"prediction\", \"avg(VAERDI)\")\n",
    "pred_tested = model.transform(test_feat).select(\"TIME_CET\",\"prediction\", \"VAERDI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_agged.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "result = pred_agged.join(pred_tested, on=\"prediction\") \\\n",
    "        .groupby(\"TIME_CET\") \\\n",
    "        .agg(sf.sum(\"avg(VAERDI)\").alias(\"predicted\"), sf.sum(\"VAERDI\").alias(\"truth\")) \\\n",
    "        .orderBy(\"TIME_CET\") \\\n",
    "        .select(\"TIME_CET\",\"predicted\",\"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=\"TIME_CET\", y=\"value\", data=pd.melt(data, ['TIME_CET']), palette=\"tab10\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}