{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37264bit3a462acd451f4b989328b3d637ce9031",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id, udf\n",
    "from pyspark.sql.types import *\n",
    "# import warning\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import schemas\n",
    "import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Methods\n",
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parquet(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    return sqlContext.read.schema(schema).parquet(path) if schema is not None else sqlContext.read.parquet(path)\n",
    "\n",
    "def load_parquet_from_weather_toPandas(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load apache parquet file\n",
    "    \"\"\"\n",
    "    weather = load_parquet(path) \\\n",
    "                    .withColumnRenamed(\"__index_level_0__\", \"TIME\") \\\n",
    "                    .dropna() \\\n",
    "                    .withColumn(\"id\", monotonically_increasing_id())\n",
    "    weather.createOrReplaceTempView(\"weather_temp\")\n",
    "    weather_dic = spark.sql(\"select * from weather_temp where id in (select max(id) as id from weather_temp group by TIME)\").toPandas()\n",
    "    return weather_dic\n",
    "\n",
    "def load_csv(path, schema:StructType = None):\n",
    "    \"\"\"\n",
    "    load csv file\n",
    "    \"\"\"\n",
    "    # return sqlContext.read.schema(schema).csv(path, sep=\";\", header=True, schema=schema) if schema is not None else sqlContext.read.schema(schema).csv(path, sep=\";\", header=True)\n",
    "    return sqlContext.read.csv(path, sep=\";\", header=True, schema=schema)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_by_grid(df:pd.DataFrame, type = FloatType()):\n",
    "    return udf(lambda g, t: list(df[df['TIME'] == t[:14]+'00:00'][g])[0], type)\n",
    "\n",
    "def udf_by_ws():\n",
    "    schema = StructType([\n",
    "        StructField(\"u_interp\", FloatType(), True),\n",
    "        StructField(\"v_interp\", FloatType(), True)\n",
    "    ])\n",
    "    return udf(lambda s1, d1, s2, d2, z: tools.wind_interp(s1, d1, s2, d2, z), schema)\n",
    "\n",
    "def udf_by_tmp():\n",
    "    schema = StructType([\n",
    "        StructField(\"tmp_interp\", FloatType(), True),\n",
    "    ])\n",
    "    return udf(lambda t1, t2, z: tools.tmp_interp(t1, t2, z), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udf_regist():\n",
    "    udf_type = udf(lambda x: {\"H\": 1.0, \"W\": 2.0, \"P\": 3.0, \"M\": 4.0}.get(x, 0.0), FloatType())\n",
    "    udf_placement = udf(lambda x: {\"LAND\": 1.0, \"HAV\": 2.0}.get(x, 0.0), FloatType())\n",
    "\n",
    "    udf_hour = udf(lambda x: int(x[11:13]), IntegerType())\n",
    "    udf_ws10  = udf_by_grid(ws10_dic, FloatType())\n",
    "    udf_ws100  = udf_by_grid(ws100_dic, FloatType())\n",
    "    udf_wd10  = udf_by_grid(wd10_dic, IntegerType())\n",
    "    udf_wd100  = udf_by_grid(wd100_dic, IntegerType())\n",
    "    # udf_tmp2  = udf_by_grid(tmp2_dic, IntegerType())\n",
    "    # udf_tmp100  = udf_by_grid(tmp100_dic, IntegerType())\n",
    "    udf_ws_interp  = udf_by_ws()\n",
    "    # udf_tmp_interp  = udf_by_tmp()\n",
    "    return udf_type, udf_placement,udf_hour, udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(df, join_df):\n",
    "    df = df.join(join_df, on=\"GSRN\") \\\n",
    "                    .select(df.GSRN, df.TIME_CET, join_df.cluster, df.VAERDI, join_df.Navhub_height, join_df.grid)\n",
    "\n",
    "    df = df.withColumn(\"hour\", udf_hour(df.TIME_CET)) \\\n",
    "                .withColumn(\"ws10\", udf_ws10(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"ws100\", udf_ws100(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"wd10\", udf_wd10(df.grid, df.TIME_CET)) \\\n",
    "                .withColumn(\"wd100\", udf_wd100(df.grid, df.TIME_CET))\n",
    "\n",
    "                # .withColumn(\"tmp2\", udf_tmp2(df.grid, df.TIME_CET)) \\\n",
    "                # .withColumn(\"tmp100\", udf_tmp100(df.grid, df.TIME_CET))\n",
    "    return df\n",
    "\n",
    "# def aggregate_with_interp(df, join_df):\n",
    "#     df = aggregate(df, join_df)\n",
    "#     df = df.withColumn(\"wsCol\", \\\n",
    "#                 udf_ws_interp(df.ws10, df.wd10, df.ws100, df.wd100, df.Navhub_height)) \\\n",
    "#         .withColumn(\"tmpCol\", \\\n",
    "#         udf_tmp_interp(df.tmp2, df.tmp100, df.Navhub_height)) \\\n",
    "#                 .select(\"GSRN\", \"TIME_CET\", \"Placement\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\", \"VAERDI\", \"wsCol.u_interp\", \"wsCol.v_interp\", \"tmpCol.tmp_interp\")\n",
    "#     return df\n",
    "\n",
    "def aggregate_with_interp(df, join_df):\n",
    "    df = aggregate(df, join_df)\n",
    "    df = df.withColumn(\"wsCol\", \\\n",
    "                udf_ws_interp(df.ws10, df.wd10, df.ws100, df.wd100, df.Navhub_height)) \\\n",
    "                .select(\"GSRN\", \"TIME_CET\", \"hour\", \"cluster\", \"VAERDI\", \"wsCol.u_interp\", \"wsCol.v_interp\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise sparkContext\\\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"WindTurbine\") \\\n",
    "    .config(\"spark.executor.memory\", \"8gb\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# using SQLContext to read parquet file\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settlement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[GSRN: string, VAERDI: float, TIME_CET: string]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    " # to read parquet file\n",
    "settlement = load_parquet(\"data/ITU_DATA/settlement/2019.parquet\", schemas.settlement_schema)\n",
    "settlement = settlement.dropna(subset =[\"VAERDI\"]) \\\n",
    "            .withColumn(\"VAERDI\", settlement[\"VAERDI\"].cast(\"float\"))\n",
    "            # .where(\"TIME_CET like '%:00:%'\")\n",
    "settlement.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- GSRN: string (nullable = true)\n |-- VAERDI: float (nullable = true)\n |-- TIME_CET: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "settlement.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weather\n",
    "from ENetNEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_10m.parquet\")\n",
    "ws100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_speed_100m.parquet\")\n",
    "wd10_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_10m.parquet\")\n",
    "wd100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/wind_direction_100m.parquet\")\n",
    "# tmp2_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/temperatur_2m.parquet\")\n",
    "# tmp100_dic = load_parquet_from_weather_toPandas(\"data/ITU_DATA/prognosis/ENetNEA/temperatur_100m.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_type, udf_placement, udf_hour, udf_ws10, udf_ws100, udf_wd10, udf_wd100, udf_ws_interp = udf_regist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windmills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[GSRN: string, Turbine_type: float, Parent_GSRN: string, BBR_municipal: string, Placement: float, UTM_x: string, UTM_y: string, Capacity_kw: float, Rotor_diameter: float, Navhub_height: float, grid: string, grid_in_range: string]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "windmill = load_csv(\"data/windmill_cleaned.csv\", schemas.windmills_schema)\n",
    "# BBR_municipal == 101  copengagen only\n",
    "windmill = windmill.where(\"grid != 0\") \\\n",
    "            .where(\"BBR_municipal == 101\") \\\n",
    "            .fillna(0.1) \\\n",
    "            .withColumn(\"Turbine_type\", udf_type(windmill.Turbine_type)) \\\n",
    "            .withColumn(\"Placement\", udf_placement(windmill.Placement))\n",
    "windmill.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windmill.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windmill.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = settlement.where(\"TIME_CET like '2019-12-30%'\")\n",
    "\n",
    "df = power.join(windmill, on=\"GSRN\") \\\n",
    "                    .select(power.GSRN, power.TIME_CET, power.VAERDI)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "result = df.groupBy(\"TIME_CET\")\\\n",
    "        .agg(sf.sum(\"VAERDI\").alias(\"sum\")) \\\n",
    "        .orderBy(\"TIME_CET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds = result.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=\"TIME_CET\", y=\"value\", data=pd.melt(pds, ['TIME_CET']), hue='variable',linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.ML Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Windmill self clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [\"Turbine_type\", \"Placement\", \"Capacity_kw\", \"Rotor_diameter\", \"Navhub_height\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "windmill = vec_assembler.transform(windmill).select(\"GSRN\", \"features\", \"Navhub_height\",\"grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(windmill)\n",
    "windmill = scalerModel.transform(windmill)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal k = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "clusters = []\n",
    "\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k, seed=1)\n",
    "    model = kmeans.fit(windmill)\n",
    "    cost.append(model.computeCost(windmill))\n",
    "    clusters.append(k)\n",
    "\n",
    "# Plot the cost\n",
    "df_cost = pd.DataFrame(cost)\n",
    "df_cost.columns = [\"cost\"]\n",
    "df_cost.insert(0, 'cluster', clusters)\n",
    "\n",
    "import pylab as pl\n",
    "pl.plot(df_cost.cluster, df_cost.cost)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DataFrame[GSRN: string, cluster: int, Navhub_height: float, grid: string]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "kmean = KMeans(featuresCol='scaledFeatures', k=5, seed=1)\n",
    "model = kmean.fit(windmill)\n",
    "windmill = model.transform(windmill).withColumnRenamed(\"prediction\", \"cluster\").select(\"GSRN\", \"cluster\",\"Navhub_height\", \"grid\")\n",
    "windmill.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicte with Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = settlement.where(\"TIME_CET not like '2019-03-31 02%'\").where(\"TIME_CET not like '2019-12-30%'\").sample(fraction=0.003, seed=5)\n",
    "test = settlement.where(\"TIME_CET like '2019-12-30%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = aggregate_with_interp(train, windmill)\n",
    "test = aggregate_with_interp(test, windmill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import serialize_torch_obj, SparkTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(\n",
    "    nn.Linear(4, 50),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(50, 50),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(50, 1),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pytorch object\n",
    "torch_obj = serialize_torch_obj(\n",
    "    model=network,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup features\n",
    "feat_cols = [\"hour\", \"cluster\", \"u_interp\", \"v_interp\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "# Create a SparkTorch Model with torch distributed. Barrier execution is on by default for this mode.\n",
    "spark_model = SparkTorch(\n",
    "    inputCol='scaledFeatures',\n",
    "    labelCol='VAERDI',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=50,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Can be used in a pipeline and saved.\n",
    "p = Pipeline(stages=[vec_assembler, scaler, spark_model]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup features\n",
    "feat_cols = [\"hour\", \"cluster\",\"u_interp\", \"v_interp\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "train = vec_assembler.transform(train).select(\"GSRN\",\"TIME_CET\", \"features\", \"VAERDI\")\n",
    "test = vec_assembler.transform(test).select(\"GSRN\",\"TIME_CET\", \"features\", \"VAERDI\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scalerModel = scaler.fit(train)\n",
    "train = scalerModel.transform(train)\n",
    "test = scalerModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.toPandas()\n",
    "test = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[[\"features\"]]\n",
    "y_train = train[[\"VAERDI\"]]\n",
    "x_test = test[[\"features\"]]\n",
    "y_test = test[[\"VAERDI\"]]\n",
    "timeline = test[[\"TIME_CET\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame([list(t) for t in x_train[\"features\"].values.tolist()], columns=[\"f1\",\"f2\",\"f3\",\"f4\"])\n",
    "x_test = pd.DataFrame([list(t) for t in x_test[\"features\"].values.tolist()], columns=[\"f1\",\"f2\",\"f3\",\"f4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.tensor(x_train.values, dtype = dtype)\n",
    "x_test_tensor = torch.tensor(x_test.values, dtype = dtype)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype = dtype)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype = dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor.shape, y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = 4\n",
    "out = 1\n",
    "\n",
    "hid = 50\n",
    "\n",
    "# nn.MSELoss() is implemented by default as: ((input-target)**2).mean()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(inp, hid),\n",
    "                            torch.nn.Tanh(),\n",
    "                            torch.nn.Linear(hid, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(10000):\n",
    "  y_pred = model(x_train_tensor)\n",
    "  loss = loss_fn(y_pred, y_train_tensor)\n",
    "\n",
    "  if iter % 1000 == 0:\n",
    "    print(iter, loss.item())\n",
    "\n",
    "  model.zero_grad()\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "      param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tensor = model(x_test_tensor)\n",
    "y_pred = y_pred_tensor.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[\"pred\"] = pd.Series(y_pred.tolist()).apply(lambda x: x[0])\n",
    "y_test[\"time\"] = pd.Series(timeline[\"TIME_CET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.groupby(y_test[\"time\"]).agg({\"VAERDI\":lambda x:sum(x), \"pred\":lambda x:sum(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.lineplot(x=\"time\", y=\"value\", data=pd.melt(y_test, ['time']), hue='variable',linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "plt.plot(y_test[\"pred\"].values, label='Predicted')\n",
    "plt.plot(y_test[\"VAERDI\"].values, label='Actual')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}